---
layout: epic
title: "Augmenting Artworks: AR at Artsy"
date: 2018-03-18
author: orta
categories: [native, ios, arkit]
comment_id: 428
---

In 2017, Apple released ARKit to universal acclaim. It's a solid foundation for application developers to build
Augmented Reality (AR) experiences without learning a whole new skillset in computer vision. Like a lot of Apple's
technology, it's a clever blend of existing projects: [SceneKit][sk], [CoreMotion][cm], [CoreML][cml] and some very
clever camera work. From the developer's perspective, ARKit has an API which fits perfectly with the rest of Apple's
APIs. You spend most of your time working with a few delegate functions and you're good to go.

For the last 2 months, I've been working with ARKit on a replacement for our [View in Room][old_vir] feature on modern
iOS devices to support a "View in _My_ Room". I'm going to try cover how we approached the project, the abstractions we
created and give you a run through of how it works.

We believe that our implementation is a solid improvement over similar features in other apps that allow users to place
artworks on walls, and we're making [the source code available][ar_vir_eigen] free and open-source under the MIT
license.

<!-- more -->

<div class="mobile-only">
<p>
  <strong>Before you get started</strong>, it looks like you're using a really small screen, 
  this post is built with larger screens in mind. You will be missing interactable elements which you <strong>really should see</strong>.
</p>
</div>

## How does ARKit work?

You start up ARKit by creating an [AR Session][ar_session] and passing it to an view on your screen. This will trigger
ARKit to open your camera looking through the phone's camera for reference points to determine where the phone is in the
world. Your app will use the reference points to attach SceneKit objects to places through the AR viewport. Let's look
at this as a diagram:

<article class="diagram">
<div style='flex:1;'>
  <ol class="workflow" data-id="What-is-AR">
    <li>You have a phone in a room</li>
    <li>ARSession starts</li>
    <li>It detects some feature points</li>
    <li>You can then attach objects to those points</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/what-is-ar.svg %}</div>
</article>

With ARKit at release in iOS 11.0, if enough feature points are detected in a series horizontally, then ARKit will also
send you some information about the what it considers to be a horizontal plane.

Planes are what most AR apps use on iOS, the apps react to discovering a flat horizontal surface and then allows putting
3d objects on to it. You would then interact with these object by doing a hit test for the closest plane or feature
point to handle user interactions.

### Quick glossary

* [Feature Point][fp] - A physical point of reference created by ARKit. A feature point is found in a frame of the
  camera's image. The point is converted into a 3D world coordinate space by image analysis that ARKit performs to track
  the device's position, orientation, and movement. Taken together, these points roughly correlate to real-world objects
  through the view of the camera.

* [Anchor][anchor] - An anchor is a developer tool for building on top of feature points. Anchors allow connections
  between a real-world object and an augmented-reality object. Effectively, this provides the API to place objects in
  "the room."

* [Plane][plane] - An anchor to a rectangle in 3D space. A set of feature points detected in a continuous direction. You
  can request for ARKit to find these for you, and common examples are tables, floors and walls.

* [Hit Test][ht] - Projects a line through the world from a location on the screen and oriented with the phone's world
  position and angle. This gives you a list of objects that intersect the line.

## AR at Artsy

We did some interesting work in the AR space already in 2017 with Microsoft, Studio Drift and the Armory Show using [the
Holo Lens][holo-lens]. When ARKit came out, we explored whether we could reliably place an artwork on a wall, but came
to the conclusion that the technology was a bit too immature for us to build it reliably. The core problem was that
ARKit at release could only be used to find horizontal planes.

Here's how our original approach worked:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v1">
    <li>Open up ARKit and let it discover the world</li>
    <li>Detect the edge of a floor by asking a user to point at the floor near the wall</li>
    <li>Find the edge furthest away</li>
    <li>Attach a work to the floor closest to the wall</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v1.svg %}</div>
</article>

This could work, but it was hard to explain for a first time user. A user spent most of their time with their phone
pointed at the floor, then we had no way to know if feature points had been detected all the way to the floor edges.

From there, we experimented with using the location of the iPhone as a point of reference:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v2">
  <li>Open up ARKit and let it discover the world</li>
  <li>Ask a user to put their phone against the wall</li>
  <li>When the camera is fully osbscured, use the last known point as an anchor</li>
  <li>When the user pulls back and reconnects, place the work</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v2.svg %}</div>
</article>

We got this to a point where it was working most of the time. One of the core issues was that there's a lot of
guess-work at the end of the process. The artwork would usually be at the position of your phone, but it was rarely
oriented correctly. We experimented by placing the artwork a meter away, but in user testing very few people could do
that roughly in their head, and we opted to use the exact position of the phone. This meant that you had to move to
touch the wall, then come back to see if the artwork was placed correctly.

This user flow was hard to explain, and it felt like we weren't setting people up to win. It was quite a debate about
whether we could ship with this or not because we wanted to ship [Quality worthy of Art][qwoa]. Then we read about the
upcoming changes in ARKit 1.5. **Vertical plane detection**. Or, _mostly_ wall detection. The debate over whether our
initial approach was good enough became moot.

With the advent of vertical wall detection, we could change our flow to be a lot like what someone would have thought it
should be:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v3">
    <li>Open up ARKit and let it discover the world</li>
    <li>Ask a user to point at a wall</li>
    <li>Use the feature detection to detect a plane</li>
    <li>Use that plane to place an artwork</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v3.svg %}</div>
</article>

This seems almost perfect, except that it only works in theory. In practice, you don't get enough feature points on a
blank wall. Recommending user point their phone at a blank wall (the kind of wall you'd place an artwork on) is likely
to end up in frustration. So instead, our user flow is:

<article class="desktop-only" style='display: flex; flex-flow:row; margin: 0px -60px;'>
<div style='flex:1;'>
  <ol class="workflow" data-id="v4">
    <li>Open up ARKit and let it discover the world</li>
    <li>Ask a user to point at an existing object on a wall</li>
    <li>Expand the discovered plane in all directions</li>
    <li>Use the extended plane to place an artwork</li>
  </ul>
</div>
<div style='flex:1'>{% include svg/ar/v4.svg %}</div>
</article>

ðŸŽ‰ - you have the Artsy workflow for putting a work on the wall.

## Abstractions

While we were not initially sure about how well ARKit would work out for us, we thought the best way to get our feet wet
would be to work with a contractor. We knew [@lazerwalker][lazerwalker] through the iOS dev community and asked if he
was interested in prototyping it out. He built out a demo application of the first design iteration, you can [see it
here][prototype1] as it's MIT licensed.

I took his prototype and [merged it into our app][ar_pr_1] Eigen. From there, I started to turn the prototype into
production code. We effectively had two state machines: one that was handling the internal state of the AR world, and
another which handled the user interface from the UIKit world. We kept these in sync by having two protocols:
`ARVIRInteractive` and `ARVIRDelegate`.

They didn't end up being that complex, you can see them in [`ARAugmentedVIRInteractionController.h`][vir_ic]. In fact,
the story of AR at Artsy is that most of this is not too complex. Which I think is a testament to how well thought-out
ARKit is.

So to make it work, we have a reasonably complicated UIViewController subclass,
[`ARAugmentedVIRViewController.m`][vir_vc] which handles presenting a user interface above the camera, then an
interaction controller [`ARAugmentedVIRInteractionController.m`][vir_ic_m] which handles all of the interactions inside
AR. By using protocols to communicate between two, I could test out different AR interactions by duplicating the
existing interaction controller and iterating on a new object for a while. This made it possible to prototype a few
ideas, then switch between them - saving on dev time as this project wasn't a good fit for [React Native][rn].

React Native is really great for [nearly every screen we deal with][rn_2] typically. We tend to basically build pretty
screens of JSON. This experience is far from that. It requires constantly changing states between two very separate but
linked worlds. One of the biggest advantages to doing it in React Native would have been using it for cross-platform
work, but that's speculative as only ARKit supports vertical plane detection today. We'd have had to either use [a
dependency][rn_ar], or build our own simpler bridge from ARKit to JS and maintain that. It just felt like a lot of work
for a one-off section of the app.

Testing this code turned out to not be too valuable, I have tests that [cover a lot of the UIKit level][tests] work but
I avoided writing tests for ARKit code. It was changing too often, and was often exploratory. Also, our tests run on iOS
10, which doesn't support ARKit anyway. So those tests would not run on CI.

## Demo

Wonder what it looks like in production? Click through for a video of it in action in my "cosy" manhattan bedroom:

<center>
<a href="http://files.artsy.net/videos/eigen-arvir-demo.mp4">
<img src="/images/ar/ar-vir-demo.png" width=325>
</a>
</center>

We explicitly aimed to try and have this project as free as possible from dependencies so that others could re-use this
code. It's not _quite_ generic enough to warrant making it's own CocoaPod, but it's MIT licensed and most of the
dependencies are to do with user-interface styling and testing, which should be easily replaced in another codebase. Let
us know if you've explored adding something it to your app or have questions!

 <style type="text/css">
.workflow li {
  width: 230px;
  padding: 20px;
  list-style:none;
  color: #80D7DA;
  border: 1px solid #80D7DA;
  margin-top:-1px;
}
.workflow li.active {
  border-left: 1px solid black;
  border-right: 1px solid black;
  color: black;
}
.diagram {
  display: flex; 
  flex-flow:row; 
  margin: 0px -60px;
}
ol {
  padding-left: 0;
  margin-top: -20px;
}
@media screen and (min-width: 900px) {
  body > div {
    background-image: url(/images/ar/cover.svg);
    background-repeat: no-repeat;
    background-position: 0px 00px;
  }
}
</style>

<script>
  $(() => {
    // When hovering on an .workflow li, use it's index
    // to disable groups in a correlating SVG. Groups in the SVG
    // all are prefixed with `start-end_name`.
    $(".workflow li").hover(function() {
        var thisIndex = $(this).index()  + 1
        var parentDataID = $(this).parent().data("id")
        $(this).parent().children().removeClass('active')
        $(this).addClass('active')

        $('g#' + parentDataID +' g').toArray().forEach(g => {
          if (!g.id.includes("-")) { return }

          var before = parseInt(g.id.split("-")[0])
          var after = parseInt(g.id.split("-")[1].split("_")[0])
          var shouldShow = before <= thisIndex && thisIndex <= after

          g.style.display = shouldShow ? "block" : "none"
        })
    });
    
    // Select the first item in every SVG li
    $(".workflow li:first-child").mouseenter()
})</script>

[holo-lens]: http://fortune.com/2017/03/02/microsoft-hololens-art-show/
[lazerwalker]: http://www.lazerwalker.com
[rn]: http://artsy.github.io/series/react-native-at-artsy/
[prototype1]: https://github.com/lazerwalker/art-on-walls-prototype
[ar_pr_1]: https://github.com/artsy/eigen/pull/2501
[vir_ic]: https://github.com/artsy/eigen/blob/f897b3438bd07470bd88a790fc6d6a524f5756cb/Artsy/View_Controllers/ARVIR/AR/ARAugmentedVIRInteractionController.h
[vir_ic_m]: https://github.com/artsy/eigen/blob/f897b3438bd07470bd88a790fc6d6a524f5756cb/Artsy/View_Controllers/ARVIR/AR/ARAugmentedVIRInteractionController.m
[vir_vc]: https://github.com/artsy/eigen/blob/f897b3438bd07470bd88a790fc6d6a524f5756cb/Artsy/View_Controllers/Core/ARViewInRoomViewController.m
[tests]: https://github.com/artsy/eigen/tree/master/Artsy_Tests/View_Controller_Tests/ARVIR
[old_vir]: https://ortastuff.s3.amazonaws.com/vids/eigen-vir.mp4
[ar_session]: https://developer.apple.com/documentation/arkit/arsession
[sk]: https://developer.apple.com/documentation/scenekit
[cm]: https://developer.apple.com/documentation/coremotion
[cml]: https://developer.apple.com/documentation/coreml
[fp]: https://developer.apple.com/documentation/arkit/arframe/2887449-rawfeaturepoints
[plane]: https://developer.apple.com/documentation/arkit/arplaneanchor
[anchor]: https://developer.apple.com/documentation/arkit/aranchor
[ht]: https://developer.apple.com/documentation/arkit/arskview/2875733-hittest
[ar_vir_eigen]: https://github.com/artsy/eigen/tree/f897b3438bd07470bd88a790fc6d6a524f5756cb/Artsy/View_Controllers/ARVIR
[rn_ar]: https://github.com/react-native-ar/react-native-arkit
[rn_2]: http://artsy.github.io/blog/2018/03/17/two-years-of-react-native/
[qwoa]: https://github.com/artsy/meta/blob/master/meta/what_is_artsy.md#quality-worthy-of-art
